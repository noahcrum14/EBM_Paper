{
  "_comment": "===== EXPERIMENT PARAMETERS =====",
  "_comment": "dataset for experiment (cifar10, cifar100, svhn)",
  "data_type": "cifar10",
  "_comment": "folder for saving experiment result",
  "exp_dir": "out_ebm/cifar10_ebm/",
  "_comment": "number of batches for ebm network weight updates",
  "num_train_batches": 250000,
  "_comment": "number of training images and synthesized images per batch",
  "batch_size": 100,
  "_comment": "standard deviation of random noise to add to training data when obtaining positive samples",
  "data_epsilon": 2e-2,
  "_comment": "seed for random numbers",
  "seed": 123,
  "_comment": "===== OPTIMIZER =====",
  "_comment": "learning rate for the initial Adam phase of learning",
  "lr_adam": 1e-4,
  "_comment": "learning rate for the final SGD phase of learning",
  "lr_sgd": 5e-5,
  "_comment": "batch to switch from Adam to SGD optimizer",
  "optimizer_switch_step": 125000,
  "_comment": "===== LANGEVIN PARAMETERS =====",
  "_comment": "step size and noise magnitude for langevin sampling",
  "epsilon": 1e-2,
  "_comment": "number of langevin steps for obtaining negatives samples during training",
  "num_shortrun_steps": 100,
  "_comment": "shortrun_init should use one of the following: persistent, data, uniform, gaussian",
  "shortrun_init": "persistent",
  "_comment": "===== DIAGNOSTIC PARAMTERS =====",
  "_comment": "frequency for saving nets and logging short-run samples",
  "log_freq": 1000,
  "_comment": "periodically obtain long-run langevin samples or not",
  "log_longrun": true,
  "_comment": "frequency of obtaining long-run langevin samples",
  "log_longrun_freq": 10000,
  "_comment": "number of langevin steps for long-run sampling",
  "num_longrun_steps": 100000,
  "_comment": "longrun_init should use one of the following: persistent, data, uniform, gaussian",
  "longrun_init": "data"
}
